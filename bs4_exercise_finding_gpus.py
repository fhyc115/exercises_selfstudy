# -*- coding: utf-8 -*-
"""BS4 exercise finding GPUs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ch7RuPv8x-pZV4HXCwcw_WONoKvzxKg
"""

pip install beautifulsoup4

from bs4 import BeautifulSoup

from google.colab import files
uploaded = files.upload()

with open("index.html", "r") as f:
  doc = BeautifulSoup(f, "html.parser")

print(doc.prettify())

tag = doc.title

print(tag)

print(tag.string)

tag.string = "hello"
print(tag)
print(doc)

tags = doc.find_all("p")

print(tags)

tagzero = doc.find_all("p")[0]

print(tagzero)

print(tagzero.find_all("b"))

import requests

# # this url doesn't work anymore b/c of bot protection builtin
# url = "https://www.newegg.ca/gigabyte-geforce-rtx-3080-ti-gv-n308tgaming-oc-12gd/p/N82E16814932436?Description=3080&cm_re=3080-_-14-932-436-_-Product"

# result = requests.get(url)
# doc = BeautifulSoup(result.text, "html.parser")
# print(doc.prettify())

## find price of gpu
# prices = doc.find_all(text = "$")
# print(prices)
# parent = prices[0].parent
# print(parent)
# strong = parent.find("strong")
# print(strong)
# print(strong.string)

"""### Searching and Filtering

"""

from google.colab import files
uploaded = files.upload()

with open("index2.html", "r") as f:
  doc = BeautifulSoup(f, "html.parser")

result = doc.find_all("option")
print(result)

# how to change selected, how to change value
tag = doc.find("option")
# access like dictionary
tag['value'] = 'new value'

print(tag)

tag['selected'] = 'false'

# add attributes
tag['color'] = 'blue'
# if we save the html document, it would aactually modify the doc

print(tag)

# if want to see what attributes are
print(tag.attrs)

# search for multiple tag names at same time
tags = doc.find_all(["p", "div", "li"])

print(tags)

# can search for combination of things at same time
# search for option tag and any tag that has Undergraduate as text for it
tags = doc.find_all(["option"], text = "Undergraduate")
print(tags)

# specific values
tags = doc.find_all(["option"], text = "Undergraduate", value = "undergraduate")
print(tags)
#tags = doc.find_all(["option"], text = "Undergraduate", value = "undergraduat")
# wouldn't work b/c value doesn't match

"""Search for Class Name

"""

# would think
# #tags = doc.find_all(["option"], text = "Undergraduate", value = "undergraduate", class = "")
# but class is reserved word in python
# instead of class, use class_ = ""
tags = doc.find_all(class_ = "btn-item")
print(tags)

"""Use regular expression re"""

import re

# use regex that will give all text left or right of $
# use re.compile to use regex
tags = doc.find_all(text = re.compile("\$.*"))
for tag in tags:
  print(tag.strip())

"""Find limit

limit the number of results 
"""

# first n results
tags = doc.find_all(text = re.compile("\$.*"), limit = 1)
for tag in tags:
  print(tag.strip())

"""Save modifications to document"""

# find all input tags that have type = text
tags = doc.find_all("input", type = "text")
for tag in tags:
  tag['placeholder'] = "I changed you!"

# now save modifications to new file
with open("changed.html", "w") as file:
  # str(doc), when take str() of doc, it gives you all the html
  # and write that into the file
  file.write(str(doc))

"""### Navigating the HTML Tree

- crypto name and price
"""

# coinmarketcap.com
import requests
url = "https://coinmarketcap.com/"
result = requests.get(url).text
doc = BeautifulSoup(result, "html.parser")

#look for tbody tag
tbody = doc.tbody
print(tbody)

"""Navigate by looking at same level on the tree
* look at siblings of same level, sibling to sibling
"""

# gives list of all tags inside of tbody, giving all trs
trs = tbody.contents
print(trs)

print(trs[0])

print(trs[0].next_sibling)

print(trs[1].previous_sibling)

# gives generator object
print(trs[0].next_siblings)

print(list(trs[0].next_siblings))

"""Tree Parents and descendants"""

# returns tbody tag
print(trs[0].parent)
print(trs[0].parent.name)

# descendants returns everything after or inside of tag
# returns all table data
# descendents give you everything, even 
print(list(trs[0].descendants))
# print(list(trs[0].content)) 
# print(list(trs[0].children))
# children will only give tags inside of tr tag

print(list(trs[0].children))

"""Traversing different levels, finding crypto prices"""

tbody = doc.tbody
trs = tbody.contents

# loop through all trs
prices = {}

for tr in trs:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  for td in tr.contents[2:4]:
    print(td)
    print()

for tr in trs:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  name, price = tr.contents[2:4]
  print(name)
  print()

for tr in trs:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  name, price = tr.contents[2:4]
  print(name.p)
  print()

# look for top 10 coins
# after 10 is nonetype so will return error
for tr in trs[:10]:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  name, price = tr.contents[2:4]
  print(name.p.string)
  print()

for tr in trs[:10]:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  name, price = tr.contents[2:4]
  fixed_name = name.p.string
  # price seems to be in a tag
  print(price)

for tr in trs[:10]:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  name, price = tr.contents[2:4]
  fixed_name = name.p.string
  # price seems to be in a tag
  print(price.a.string)

for tr in trs[:10]:
  # look through td in tr
  # [2:4] returns name and price, 2nd and 3rd piece of info
  name, price = tr.contents[2:4]
  fixed_name = name.p.string
  # price seems to be in a tag
  fixed_price = price.a.string

  prices[fixed_name] = fixed_price

print(prices)

"""If we want to apply to all other trs, 
* find what happens after 10
* find rest in a different way?
* different strategy that doesn't just look for the p tag and look for specific amount of text or a $
"""

# fix for nonetype after 10

for tr in trs:
  name, price = tr.contents[2:4]
  if name.p is None:
    pass
  else:
    fixed_name = name.p.string

  if price.a is None:
    pass
  else:
    fixed_price = price.a.string
  
  prices[fixed_name] = fixed_price

print(prices)

"""### Finding GPU prices from multiple websites"""

from bs4 import BeautifulSoup
import requests
import re

#user inputs what product they want to search for
search_term = input("What product(GPU) do you want to search for? ")

# d = search, N = 4131 means available in stock
url = f"https://www.newegg.ca/p/pl?d={search_term}&N=4131"
page = requests.get(url).text
doc = BeautifulSoup(page, "html.parser")
page_text = doc.find(class_ = "list-tool-pagination-text").strong
pages = int(str(page_text).split("/")[-2].split(">")[-1][:-1])
print(pages)

"""How many pages do we have?
* want to look on all of the pages
* find text on the bottom, find number and loop through all pages
* span class = "list-tool-pagination-text"
* strong
* turn doc.find(class_ = "list-tool-pagination-text").strong into a string
* split string at / and get 2nd last element
  * returns '<!-- -->4<'
  * don't know if page number is more than 1 digit or not
  * can't just grab index, need to split string again at '>', and remove very last character

Now that we have the pages,
"""

items_found = {}
# don't want to start at page 0 in a for loop and range(n) it starts at 0
# start at 1 and go up to last page b/c last number is exclusive
for page in range(1, pages + 1):
  # send another request by adding page={page}
  url = f"https://www.newegg.ca/p/pl?d={search_term}&N=4131&page={page}"
  page = requests.get(url).text 
  doc = BeautifulSoup(page, "html.parser")
  div = doc.find(class_ = "item-cells-wrap border-cells items-grid-view four-cells expulsion-one-cell")
  #grab every single item that says input(text)
  # change from doc.find_all to div.find_all, so that we're searching within the div and not the doc
  # getting more concentrated/filtered results
  items = div.find_all(text = re.compile(search_term))
  for item in items:
    # what we want to grab is the href
    parent = item.parent
    link = None
    if parent.name != "a":
      continue

    link = parent['href']
    next_parent = item.find_parent(class_ = "item-container")
    try:
      price = next_parent.find(class_ = "price-current").find("strong").string
      items_found[item] = {"price": int(price.replace(",", "")), "link": link}
    except:
      pass
  
# .items() returns a tuple that has keys and then the value
# sort with following function lambda x: x[1]['price']
# create all tuples that contain key and value, value being the dict having price and link inside
# passing function that we want to sort this
# x will be all of these items, x[1] gives 2nd item which will be the dictionary, access the price
sorted_items = sorted(items_found.items(), key = lambda x: x[1]['price'])

# loop through sorted_items and print them out
for item in sorted_items:
  # name, the key
  print(item[0])
  # returns price, returns link
  print(f"${item[1]['price']}")
  print(item[1]['link'])
  print('------------------------------------------')

"""Only want items inside a specific div class
* because we only want actual items, instead of random text that also matches the re.compile
* returns only text and not random text/stuff that we don't need
* now look at parent of this text, is a link href
* some results won't have parent tag, "a" tag, giving an error
* make sure parent is an a tag before we try to return stuff
* potentially getting none in return b/c 3080 or the input is an attribute of some tag
* if parent tag is not "a" tag, don't bother adding it in, skip it
* now need price: item-container -> item-action -> price-current

Sort all items by price, and have nicer output
* sort dictionary: 
  * convert to list of some sort
  * sort list
  * convert back to dictionary

* items_found.items() returns a list of tuples
* [("3080 FTW", {'price': 2999, "link": "https://..."})]
* few entries don't have strong field
  * to fix this, use try and except? if can't find string from strong
"""